# ğŸ¬ IMDB Movie Reviews â€” LSTM Sentiment Classifier

An end-to-end deep learning project that builds a **binary sentiment classifier** on the [IMDB dataset](https://keras.io/api/datasets/imdb/) using **Keras + TensorFlow**.  
The model classifies reviews as **positive (1)** or **negative (0)** using an **Embedding â†’ LSTM â†’ Dense(sigmoid)** architecture.  

---

## ğŸ“Œ Features
- âœ… Uses `keras.datasets.imdb` (50k reviews; pre-tokenized, labeled data)  
- âœ… Preprocessing with **tokenization + padding** (`pad_sequences`)  
- âœ… **LSTM-based model** for sequential text classification  
- âœ… Training history visualization (accuracy & loss curves)  
- âœ… **Example predictions** on held-out test reviews  
- âœ… Utility to decode word indices back to raw text  

---

## ğŸ“‚ Project Structure
notebooks/
â””â”€â”€ imdb_lstm_sentiment.ipynb   # Main notebook
README.md                        # Project documentation
requirements.txt                 # Minimal dependencies
---

## âš™ï¸ Environment
- Python 3.9+  
- TensorFlow 2.12+ (or compatible)  
- NumPy, Matplotlib, scikit-learn  

ğŸ“Š Dataset
	â€¢	Loaded via:
from keras.datasets import imdb
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)
Train/Test split: 25k / 25k reviews
â€¢	Preprocessing:
	â€¢	Vocabulary limited to vocab_size (default: 20,000)
	â€¢	Pad/truncate sequences to maxlen (default: 250)
 from keras.preprocessing.sequence import pad_sequences
x_train = pad_sequences(x_train, maxlen=250, padding="post", truncating="post")
ğŸ§  Model Architecture
Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen)
LSTM(128, dropout=0.2, recurrent_dropout=0.2)
Dense(1, activation='sigmoid')
Compilation 
loss='binary_crossentropy'
optimizer='adam'
metrics=['accuracy']
Training
	â€¢	batch_size=64, epochs=8
	â€¢	validation_split=0.2
	â€¢	Early stopping with patience=2
 
 ğŸ“ˆ Results

Typical performance (varies with hyperparameters & hardware):
	â€¢	Training Accuracy: 88â€“92%
	â€¢	Test Accuracy: 85â€“90%

The notebook prints:
	â€¢	Training/validation accuracy and loss per epoch
	â€¢	Final test accuracy
	â€¢	Example decoded reviews with predicted sentiment

ğŸš€ Example Usage

Run the notebook locally or on Google Colab: jupyter notebook notebooks/imdb_lstm_sentiment.ipynb
You will be able to:
	1.	Load and preprocess IMDB dataset
	2.	Train and evaluate the LSTM model
	3.	Visualize accuracy & loss curves
	4.	Run predictions on example reviews
 ğŸ› Customization
	â€¢	Model: Try Bidirectional(LSTM(...)) for stronger performance
	â€¢	Hyperparameters: Adjust vocab_size, maxlen, embedding_dim, lstm_units, batch_size
	â€¢	Regularization: Increase dropout, add L2 penalty
	â€¢	Visualization: Add confusion matrix & classification report with scikit-learn
	â€¢	Checkpointing: Save best model weights with ModelCheckpoint
 âš ï¸ Limitations
	â€¢	The IMDB dataset is pre-indexed (not raw text). Using pretrained embeddings (GloVe, Word2Vec) requires custom tokenization.
	â€¢	LSTMs are slower than CNNs or Transformers; performance depends on hardware and tuning.

â¸»

ğŸ™ Acknowledgments
	â€¢	Keras/TensorFlow team for the IMDB dataset loader and deep learning APIs.
